import io
from typing import Dict, Union

import arviz as az
import gradio as gr
import japanize_matplotlib  # noqa: F401
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pymc as pm
import statsmodels.stats.power as smp
import statsmodels.stats.proportion as smprop
from PIL import Image
from scipy.stats import chi2_contingency
from tqdm import tqdm

from src.models.base import define_model
from src.visualization.visualize import plot_distribution


def calculate_sample_size(
    baseline_cvr: float,
    lift: float,
    alpha: float = 0.05,
    power: float = 0.80,
    ratio: float = 1.0,
) -> Dict[str, Union[float, int]]:
    """
    A/Bãƒ†ã‚¹ãƒˆã®CVRã«åŸºã¥ã„ã¦å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã—ã€çµæœã‚’è¾æ›¸ã§è¿”ã—ã¾ã™ã€‚

    Args:
        baseline_cvr (float): ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®CVRï¼ˆä¾‹: 0.03ï¼‰ã€‚
        lift (float): æ¤œå‡ºã—ãŸã„CVRã®çµ¶å¯¾çš„ãªæ”¹å–„é‡ï¼ˆä¾‹: 0.005ï¼‰ã€‚
        alpha (float, optional): æœ‰æ„æ°´æº–ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 0.05ã€‚
        power (float, optional): æ¤œå‡ºåŠ›ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 0.80ã€‚
        ratio (float, optional): ã‚°ãƒ«ãƒ¼ãƒ—Bã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º / ã‚°ãƒ«ãƒ¼ãƒ—Aã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®æ¯”ç‡ã€‚
                                 ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 1.0 (å‡ç­‰)ã€‚

    Returns:
        Dict[str, Union[float, int]]: è¨ˆç®—çµæœã‚’å«ã‚€è¾æ›¸ã€‚
            - baseline_cvr: å…¥åŠ›ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³CVR
            - target_cvr: ç›®æ¨™CVR (baseline_cvr + lift)
            - lift: å…¥åŠ›ã•ã‚ŒãŸæ”¹å–„é‡
            - alpha: æœ‰æ„æ°´æº–
            - power: æ¤œå‡ºåŠ›
            - ratio: ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºæ¯”ç‡
            - effect_size: è¨ˆç®—ã•ã‚ŒãŸåŠ¹æœé‡ (Cohen's h)
            - required_sample_size_group_a: ã‚°ãƒ«ãƒ¼ãƒ—Aã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
            - required_sample_size_group_b: ã‚°ãƒ«ãƒ¼ãƒ—Bã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
            - total_sample_size: åˆè¨ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
    """
    # åŠ¹æœé‡ï¼ˆEffect Sizeï¼‰ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
    effect_size = smprop.proportion_effectsize(
        baseline_cvr, baseline_cvr + lift
    )

    # ã‚°ãƒ«ãƒ¼ãƒ—Aã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆnobs1ï¼‰ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
    # ratioã¯ nobs2/nobs1 ã¨ã—ã¦æ‰±ã‚ã‚Œã¾ã™ã€‚
    required_nobs1 = smp.NormalIndPower().solve_power(
        effect_size=effect_size,
        power=power,
        alpha=alpha,
        ratio=ratio,
        alternative="two-sided",  # ä¸¡å´æ¤œå®š
    )

    # è¨ˆç®—çµæœã‚’æ•´æ•°ã«åˆ‡ã‚Šä¸Šã’ã¾ã™ã€‚
    nobs1 = int(np.ceil(required_nobs1))
    nobs2 = int(np.ceil(nobs1 * ratio))

    # çµæœã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
    result_dict = {
        "baseline_cvr": baseline_cvr,
        "target_cvr": baseline_cvr + lift,
        "lift": lift,
        "alpha": alpha,
        "power": power,
        "ratio": ratio,
        "effect_size": effect_size,
        "required_sample_size_group_a": nobs1,
        "required_sample_size_group_b": nobs2,
        "total_sample_size": nobs1 + nobs2,
    }

    return result_dict


def run_chisquared_test(n_a, conversion_a, n_b, conversion_b) -> pd.DataFrame:
    """
    A/Bãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚«ã‚¤äºŒä¹—æ¤œå®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

    Parameters
    ----------
    n_a, conversion_a : int
        Aç¾¤ã®è©¦è¡Œå›æ•°ã¨ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°ã€‚
    n_b, conversion_b : int
        Bç¾¤ã®è©¦è¡Œå›æ•°ã¨ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°ã€‚

    Returns
    -------
    pd.DataFrame
        ã‚«ã‚¤äºŒä¹—æ¤œå®šã®çµæœï¼ˆã‚«ã‚¤äºŒä¹—å€¤ã€på€¤ã€è‡ªç”±åº¦ï¼‰ã‚’æ ¼ç´ã—ãŸDataFrameã€‚
    """
    # 2x2ã®åˆ†å‰²è¡¨ã‚’ä½œæˆ
    #        | Conversion | No Conversion |
    # -------|------------|---------------|
    # Group A| conv_a     | n_a - conv_a  |
    # Group B| conv_b     | n_b - conv_b  |
    table = np.array(
        [
            [conversion_a, n_a - conversion_a],
            [conversion_b, n_b - conversion_b],
        ]
    )

    chi2, p, dof, _ = chi2_contingency(table, correction=False)

    return pd.DataFrame(
        {
            "æŒ‡æ¨™": ["ã‚«ã‚¤äºŒä¹—å€¤", "på€¤", "è‡ªç”±åº¦"],
            "å€¤": [f"{chi2:.4f}", f"{p:.4f}", dof],
        }
    )


def analyze_bayesian_results(model: pm.Model, trace, hdi_prob: float) -> tuple:
    """
    ãƒ™ã‚¤ã‚ºåˆ†æã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœã‚’åˆ†æã—ã€å¯è¦–åŒ–ã®ãŸã‚ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Parameters
    ----------
    model : pm.Model
        PyMCãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    trace : az.InferenceData
        ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœã®ãƒˆãƒ¬ãƒ¼ã‚¹ã€‚
    hdi_prob : float
        æœ€é«˜äº‹å¾Œå¯†åº¦åŒºé–“ï¼ˆHDIï¼‰ã®ç¢ºç‡ã€‚

    Returns
    -------
    tuple
        å¯è¦–åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆã®ã‚¿ãƒ—ãƒ«:
        (åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ, ãƒ¢ãƒ‡ãƒ«æ§‹é€ , ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ, ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ—ãƒ­ãƒƒãƒˆ)
    """
    # UIã‹ã‚‰ã¯çœŸã®å€¤ã‚„è¦³æ¸¬å€¤ã®å¹³å‡ã¯ãƒ—ãƒ­ãƒƒãƒˆã—ãªã„ãŸã‚ã€æœ€å°é™ã®dataè¾æ›¸ã‚’ä½œæˆ
    data_for_plot = {"p_a_true": None, "p_b_true": None}
    dist_plot = plot_distribution(data_for_plot, trace)

    # model arch
    png_data = pm.model_to_graphviz(model).pipe(format="png")
    model_arch = Image.open(io.BytesIO(png_data))

    # trace plot
    trace_plot, axes = plt.subplots(4, 2, figsize=(12, 8))
    pm.plot_trace(trace, compact=False, axes=axes)
    trace_plot.tight_layout()
    for ax in axes.flatten():
        ax.grid()

    # forest
    var_names = trace.posterior.data_vars.keys()
    var_length = len(var_names)
    forest, axes = plt.subplots(
        var_length,
        2,
        figsize=(12, var_length * 3),
    )
    for index, var_name in enumerate(var_names):
        az.plot_forest(
            trace,
            var_names=[var_name],
            combined=True,
            r_hat=True,
            hdi_prob=hdi_prob,
            ax=axes[index],
        )
    forest.tight_layout()
    for ax in axes.flatten():
        ax.grid()

    return dist_plot, model_arch, trace_plot, forest


def run_bayesian_analysis(
    n_a,
    conversion_a,
    n_b,
    conversion_b,
    n_sampling,
    n_tune,
    n_chains,
    random_seed,
    hdi_prob,
    progress=gr.Progress(track_tqdm=True),
):
    """
    Gradio UIã‹ã‚‰ã®å…¥åŠ›ã«åŸºã¥ã„ã¦ã€A/Bãƒ†ã‚¹ãƒˆã®ãƒ™ã‚¤ã‚ºåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

    Parameters
    ----------
    (çœç•¥)

    Returns
    -------
    tuple
        analyze_bayesian_resultsé–¢æ•°ã‹ã‚‰è¿”ã•ã‚Œã‚‹ãƒ—ãƒ­ãƒƒãƒˆã®ã‚¿ãƒ—ãƒ«ã€‚
    """

    # ãƒ¢ãƒ‡ãƒ«å®šç¾©
    model = define_model([n_a, n_b], [conversion_a, conversion_b])

    with model:
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¨­å®š
        pbar = tqdm(total=n_chains * (n_sampling + n_tune), desc="ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸­")

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
        def update_progress(trace, draw):
            pbar.update(1)

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        trace = pm.sample(
            draws=n_sampling,
            tune=n_tune,
            chains=n_chains,
            random_seed=random_seed,
            callback=update_progress,
        )

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚¯ãƒ­ãƒ¼ã‚º
        pbar.close()

    # åˆ†æã¨å¯è¦–åŒ–
    return analyze_bayesian_results(model, trace, hdi_prob)


def run_power_analysis(baseline_cvr, lift, alpha, power, ratio):
    test_params = {
        "baseline_cvr": baseline_cvr,
        "lift": lift,
        "alpha": alpha,
        "power": power,
        "ratio": ratio,  # ä¾‹: ã‚°ãƒ«ãƒ¼ãƒ—Aã¨Bã‚’1:1ã«ã™ã‚‹å ´åˆã¯1.0ã€A:B=1:2ã«ã™ã‚‹å ´åˆã¯2.0
    }
    result = calculate_sample_size(**test_params)
    result_df = pd.DataFrame([result]).T.reset_index()
    result_df.columns = ["æŒ‡æ¨™", "å€¤"]
    return (
        result["required_sample_size_group_a"],
        result["required_sample_size_group_b"],
        result_df,
    )


text_about_this_tool = """
ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€A/Bãƒ†ã‚¹ãƒˆã®çµæœã‚’ã€Œãƒ™ã‚¤ã‚ºçµ±è¨ˆã€ã¨ã€Œé »åº¦è«–çµ±è¨ˆï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰ã€ã®2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§åˆ†æã—ã€æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- **ãƒ™ã‚¤ã‚ºåˆ†æ**: æ–½ç­–ã®å„ªåŠ£ã‚’ã€Œç¢ºç‡ã€ã¨ã—ã¦è§£é‡ˆã—ã€æŸ”è»Ÿãªæ„æ€æ±ºå®šã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
- **ã‚«ã‚¤äºŒä¹—æ¤œå®š**: å¤å…¸çš„ãªçµ±è¨ˆæ‰‹æ³•ã§ã€2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—é–“ã«ã€Œçµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã€ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’på€¤ã§åˆ¤æ–­ã—ã¾ã™ã€‚
"""

text_about_baysian_analysis = """
#### åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆã®è¦‹æ–¹
- **p_a, p_bã®åˆ†å¸ƒ**: ãã‚Œãã‚Œã®ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ãŒã€ã©ã®å€¤ã‚’å–ã‚Šãã†
ã‹ã¨ã„ã†ã€Œç¢ºä¿¡åº¦ã€ã‚’åˆ†å¸ƒã§è¡¨ã—ã¦ã„ã¾ã™ã€‚å¹…ãŒç‹­ã„ã»ã©ã€æ¨å®šã®ç¢ºä¿¡åº¦ãŒé«˜ã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
- **uplift (p_b - p_a)ã®åˆ†å¸ƒ**: BãŒAã‚ˆã‚Šã©ã‚Œã ã‘è‰¯ã„ã‹ï¼ˆçµ¶å¯¾å·®ï¼‰ã®åˆ†å¸ƒã§ã™ã€‚
    - åˆ†å¸ƒå…¨ä½“ãŒ0ã‚ˆã‚Šå¤§ãã‘ã‚Œã°ã€ã€ŒBã¯Aã‚ˆã‚Šè‰¯ã„ã€ã¨å¼·ãè¨€ãˆãã†ã§ã™ã€‚
    - 0ã‚’ã¾ãŸã„ã§ã„ã‚‹å ´åˆã€å„ªåŠ£ã®åˆ¤æ–­ã¯ä¸ç¢ºå®Ÿã§ã™ã€‚åˆ†å¸ƒãŒ0ã‚ˆã‚Šå¤§ãã„éƒ¨åˆ†ã®é¢ç©ãŒã€ŒBãŒAã‚’ä¸Šå›ã‚‹ç¢ºç‡ã€ã«ç›¸å½“ã—ã¾ã™ã€‚
- **relative_uplift ((p_b - p_a)/p_a)ã®åˆ†å¸ƒ**: æ”¹å–„ç‡ã®åˆ†å¸ƒã§ã™ã€‚ã€ŒCVRãŒä½•%æ”¹å–„ã—ãŸã‹ã€ã‚’ç¢ºç‡çš„ã«è§£é‡ˆã§ãã¾ã™ã€‚
#### ã“ã‚“ãªæ™‚ã«ä¾¿åˆ©
- ã€ŒBãŒAã‚ˆã‚Šè‰¯ã„ç¢ºç‡ã¯ï¼Ÿã€ã¨ã„ã£ãŸå•ã„ã«ç›´æ¥ç­”ãˆãŸã„æ™‚ã€‚
- ãƒ†ã‚¹ãƒˆæœŸé–“ãŒçŸ­ãã€ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªãã¦ã‚‚ã€æš«å®šçš„ãªç¤ºå”†ã‚’å¾—ãŸã„æ™‚ã€‚
- ã€Œæœ‰æ„å·®ãªã—ã€ã¨ã„ã†çµæœã ã‘ã§ãªãã€å„ªåŠ£ã®ç¢ºç‡ã‚’çŸ¥ã‚Šã€ãƒ“ã‚¸ãƒã‚¹åˆ¤æ–­ã«æ´»ã‹ã—ãŸã„æ™‚ã€‚
"""

text_about_chi_squared_test = """
#### på€¤ã®è§£é‡ˆ
- **på€¤**ã¯ã€ã€Œã‚‚ã—Aã¨Bã®é–“ã«æœ¬å½“ã¯å·®ãŒãªã„ã¨ã—ãŸã‚‰ã€ä»Šå›è¦³æ¸¬ã•ã‚ŒãŸã‚ˆã†ãªå·®ï¼ˆã¾ãŸã¯ãã‚Œä»¥ä¸Šã®å·®ï¼‰ãŒå¶ç„¶ç”Ÿã˜ã‚‹ç¢ºç‡ã€ã‚’ç¤ºã—ã¾ã™ã€‚
- æ…£ç¿’çš„ã«ã€på€¤ãŒ0.05ï¼ˆ5%ï¼‰ã‚’ä¸‹å›ã‚‹å ´åˆã«ã€Œçµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãŒã‚ã‚‹ã€ã¨åˆ¤æ–­ã—ã€ã€ŒAã¨Bã®ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã«ã¯å·®ãŒã‚ã‚‹ã€ã¨çµè«–ä»˜ã‘ã¾ã™ã€‚
- på€¤ãŒ0.05ã‚’ä¸Šå›ã‚‹å ´åˆã¯ã€ã€Œçµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãŒã‚ã‚‹ã¨ã¯è¨€ãˆãªã„ã€ã¨çµè«–ä»˜ã‘ã¾ã™ã€‚ã“ã‚Œã¯ã€Œå·®ãŒãªã„ã€ã“ã¨ã‚’è¨¼æ˜ã™ã‚‹ã‚‚ã®ã§ã¯ãªã„ç‚¹ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚
#### ã“ã‚“ãªæ™‚ã«ä¾¿åˆ©
- ä¼çµ±çš„ãªçµ±è¨ˆçš„ä»®èª¬æ¤œå®šã®æ çµ„ã¿ã§ã€å®¢è¦³çš„ãªã€Œæœ‰æ„å·®ã€ã®æœ‰ç„¡ã‚’åˆ¤æ–­ã—ãŸã„æ™‚ã€‚
- ãƒ¬ãƒãƒ¼ãƒˆãªã©ã§ã€åºƒãå—ã‘å…¥ã‚Œã‚‰ã‚Œã¦ã„ã‚‹på€¤ã‚’å ±å‘Šã™ã‚‹å¿…è¦ãŒã‚ã‚‹æ™‚ã€‚
#### æ³¨æ„ç‚¹: pãƒãƒƒã‚­ãƒ³ã‚°ã¨æ¤œå®šã®æ£æ„çš„ãªåœæ­¢
- **pãƒãƒƒã‚­ãƒ³ã‚°**: på€¤ãŒ0.05ã‚’ä¸‹å›ã‚‹ã¾ã§ã€åˆ†ææ–¹æ³•ã‚’è‰²ã€…è©¦ã—ãŸã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
ã—ãŸã‚Šã€é€†ã«ä¸€éƒ¨ã‚’é™¤å¤–ã—ãŸã‚Šã™ã‚‹è¡Œç‚ºã¯ã€Œpãƒãƒƒã‚­ãƒ³ã‚°ã€ã¨å‘¼ã°ã‚Œã€èª¤ã£ãŸçµè«–ã‚’å°ãåŸå› ã¨ãªã‚Šã¾ã™ã€‚
- **æ£æ„çš„ãªæ¤œå®šåœæ­¢**: ãƒ†ã‚¹ãƒˆã®é€”ä¸­ã§på€¤ãŒ0.05ã‚’ä¸‹å›ã£ãŸã‹ã‚‰ã¨ã„ã£ã¦ã€ãã“ã§ãƒ†ã‚¹ãƒˆã‚’çµ‚äº†ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
ã“ã‚Œã¯ã€Œå½é™½æ€§ï¼ˆæœ¬å½“ã¯å·®ãŒãªã„ã®ã«ã€å·®ãŒã‚ã‚‹ã¨åˆ¤æ–­ã—ã¦ã—ã¾ã†ã“ã¨ï¼‰ã€ã®ç¢ºç‡ã‚’é«˜ã‚ã‚‹è¡Œç‚ºã§ã™ã€‚ãƒ†ã‚¹ãƒˆã¯ã€
äº‹å‰ã«æ±ºã‚ãŸã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«é”ã™ã‚‹ã¾ã§ç¶šã‘ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
"""

text_about_power_analysis = """
**æ¤œå‡ºåŠ›åˆ†æ (Power Analysis)** ã¯ã€A/Bãƒ†ã‚¹ãƒˆã‚’**å®Ÿæ–½ã™ã‚‹å‰**ã«è¡Œã†ã€é©åˆ‡ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¦‹ç©ã‚‚ã‚‹ãŸã‚ã®çµ±è¨ˆçš„ãªæ‰‹æ³•ã§ã™ã€‚

#### ãªãœå¿…è¦ã‹ï¼Ÿ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹ã¨ã€æœ¬å½“ã«åŠ¹æœã®ã‚ã‚‹æ–½ç­–ã§ã‚‚ã€Œå·®ãŒãªã„ã€ã¨ã„ã†èª¤ã£ãŸçµè«–ã«è‡³ã‚‹ãƒªã‚¹ã‚¯ï¼ˆå½é™°æ€§ï¼‰ãŒé«˜ã¾ã‚Šã¾ã™ã€‚
- é€†ã«ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹ã¨ã€å®Ÿå‹™çš„ã«ã¯æ„å‘³ã®ãªã„ã”ãåƒ…ã‹ãªå·®ã§ã‚‚ã€Œçµ±è¨ˆçš„ã«æœ‰æ„ã€ã¨ã„ã†çµæœã«ãªã‚Šã‚„ã™ããªã‚‹ä¸Šã€
æ™‚é–“ã‚„ã‚³ã‚¹ãƒˆã‚‚ç„¡é§„ã«ã‹ã‹ã£ã¦ã—ã¾ã„ã¾ã™ã€‚æ¤œå‡ºåŠ›åˆ†æã¯ã€ã“ã‚Œã‚‰ã®ãƒªã‚¹ã‚¯ã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ãŸã‚ã«å½¹ç«‹ã¡ã¾ã™ã€‚

#### ä¸»è¦ãª4ã¤ã®è¦ç´ 
1.  **æ¤œå‡ºåŠ› (Power, 1-Î²)**: æœ¬å½“ã«å·®ãŒã‚ã‚‹æ™‚ã«ã€ãã‚Œã‚’ã€Œå·®ãŒã‚ã‚‹ã€ã¨æ­£ã—ãæ¤œå‡ºã§ãã‚‹ç¢ºç‡ã€‚ä¸€èˆ¬çš„ã«80% (0.8) ã«è¨­å®šã•ã‚Œã¾ã™ã€‚
2.  **æœ‰æ„æ°´æº– (Significance Level, Î±)**: æœ¬å½“ã¯å·®ãŒãªã„ã®ã«ã€ã€Œå·®ãŒã‚ã‚‹ã€ã¨é–“é•ã£ã¦åˆ¤æ–­ã—ã¦ã—ã¾ã†ç¢ºç‡ï¼ˆå½é™½æ€§ï¼‰ã€‚
é€šå¸¸ã€5% (0.05) ã«è¨­å®šã•ã‚Œã¾ã™ã€‚
3.  **åŠ¹æœé‡ (Effect Size)**: æ¤œå‡ºã—ãŸã„å·®ã®å¤§ãã•ï¼ˆä¾‹: CVRãŒ2%ã‹ã‚‰2.5%ã«æ”¹å–„ï¼‰ã€‚åŠ¹æœé‡ãŒå°ã•ã„ã»ã©ã€
ãã‚Œã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã«ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚
4.  **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º (Sample Size)**: å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã‚„è©¦è¡Œå›æ•°ã€‚

ã“ã®ãƒ„ãƒ¼ãƒ«ã§ã¯æ¤œå‡ºåŠ›åˆ†æã¯è¡Œãˆã¾ã›ã‚“ãŒã€A/Bãƒ†ã‚¹ãƒˆã‚’è¨ˆç”»ã™ã‚‹éš›ã«ã¯ã€ã“ã‚Œã‚‰ã®æ¦‚å¿µã‚’ç†è§£ã—ã€å°‚ç”¨ã®è¨ˆç®—ãƒ„ãƒ¼ãƒ«ãªã©ã‚’ä½¿ã£ã¦äº‹å‰ã«ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¦‹ç©ã‚‚ã‚‹ã“ã¨ãŒéå¸¸ã«é‡è¦ã§ã™ã€‚
"""

text_bayesien_vs_chi_squared = """
| ç‰¹å¾´ | ãƒ™ã‚¤ã‚ºåˆ†æ | ã‚«ã‚¤äºŒä¹—æ¤œå®š |
|:---|:---|:---|
| **å¾—ã‚‰ã‚Œã‚‹çµæœ** | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºç‡åˆ†å¸ƒ | på€¤ |
| **è§£é‡ˆã®ä»•æ–¹** | ã€ŒBãŒAã‚ˆã‚Šè‰¯ã„ç¢ºç‡ã¯X%ã€ | ã€Œæœ‰æ„å·®ãŒã‚ã‚‹/ãªã„ã€ |
| **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**| å°ã•ãã¦ã‚‚ç¤ºå”†ã¯å¾—ã‚‰ã‚Œã‚‹ | ã‚ã‚‹ç¨‹åº¦ã®å¤§ãã•ãŒå¿…è¦ |
| **æŸ”è»Ÿæ€§** | äº‹å‰çŸ¥è­˜ã‚’ãƒ¢ãƒ‡ãƒ«ã«çµ„è¾¼å¯èƒ½ | å›°é›£ |
| **è¨ˆç®—ã‚³ã‚¹ãƒˆ** | é«˜ã„ï¼ˆMCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ | éå¸¸ã«ä½ã„ |

**ä½¿ã„åˆ†ã‘ã®ãƒ’ãƒ³ãƒˆ**:
- **ã‚«ã‚¤äºŒä¹—æ¤œå®š**ã§ã€Œãã‚‚ãã‚‚å·®ãŒã‚ã‚‹ã¨è¨€ãˆã‚‹ã®ã‹ï¼Ÿã€ã‚’ç´ æ—©ãç¢ºèªã—ã€
- **ãƒ™ã‚¤ã‚ºåˆ†æ**ã§ã€Œã§ã¯ã€ã©ã®ãã‚‰ã„å·®ãŒã‚ã‚Šãã†ã‹ï¼Ÿã€ã€ŒBã‚’é¸ã¶ã¹ãç¢ºç‡ã¯ï¼Ÿã€ã¨ã„ã£ãŸã€ã‚ˆã‚Šè¸ã¿è¾¼ã‚“ã æ„æ€æ±ºå®šã‚’è¡Œã†ã€ã¨ã„ã†ä½¿ã„åˆ†ã‘ãŒæœ‰åŠ¹ã§ã™ã€‚
"""


# Gradio UIã®å®šç¾©
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# A/Bãƒ†ã‚¹ãƒˆåˆ†æãƒ„ãƒ¼ãƒ« (ãƒ™ã‚¤ã‚ºæ¨å®š vs ã‚«ã‚¤äºŒä¹—æ¤œå®š)")

    with gr.Accordion("â“ ã“ã®ãƒ„ãƒ¼ãƒ«ã§ä½•ãŒã§ãã‚‹ã‹", open=False):
        gr.Markdown(text_about_this_tool)

    # --- å…¥åŠ›ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ ---
    gr.Markdown("## 1. ãƒ†ã‚¹ãƒˆçµæœã®å…¥åŠ›")
    with gr.Row():
        with gr.Column():
            gr.Markdown("### Aç¾¤")
            n_a = gr.Number(1000, label="è©¦è¡Œå›æ•° (ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º)")
            conversion_a = gr.Number(53, label="ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°")
        with gr.Column():
            gr.Markdown("### Bç¾¤")
            n_b = gr.Number(100, label="è©¦è¡Œå›æ•° (ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º)")
            conversion_b = gr.Number(10, label="ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°")

    with gr.Accordion("âš™ï¸ ãƒ™ã‚¤ã‚ºåˆ†æã®è©³ç´°è¨­å®š", open=False):
        n_sampling = gr.Number(4000, label="ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•° (draws)")
        n_tune = gr.Number(1000, label="ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å›æ•° (tune)")
        n_chains = gr.Number(4, label="ãƒã‚§ãƒ¼ãƒ³æ•°")
        hdi_prob = gr.Number(
            0.95, label="Highest Density Interval (HDI:æœ€é«˜äº‹å¾Œå¯†åº¦åŒºé–“)"
        )
        random_seed = gr.Number(1234, label="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰")

    start_button = gr.Button("åˆ†æé–‹å§‹", variant="primary")

    # --- å‡ºåŠ›ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ ---
    gr.Markdown("## 2. åˆ†æçµæœ")
    with gr.Row():
        with gr.Column(scale=5):
            gr.Markdown("### ãƒ™ã‚¤ã‚ºåˆ†æã®çµæœ")
            with gr.Tabs():
                with gr.TabItem("ğŸ“ˆ åˆ†å¸ƒ"):
                    distribution = gr.Plot()
                with gr.TabItem("ğŸ“Š ã‚µãƒãƒªãƒ¼"):
                    params = gr.Plot()
                with gr.TabItem("ğŸ•¸ï¸ ãƒ¢ãƒ‡ãƒ«"):
                    model_img = gr.Image()
                with gr.TabItem("ğŸ›°ï¸ ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ"):
                    trace_plot = gr.Plot()
        with gr.Column(scale=1):
            gr.Markdown("### ã‚«ã‚¤äºŒä¹—æ¤œå®šã®çµæœ")
            chisq_result_df = gr.DataFrame(
                headers=["æŒ‡æ¨™", "å€¤"], datatype=["str", "str"], label="æ¤œå®šçµæœ"
            )

    # --- è§£èª¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
    with gr.Accordion("ğŸ’¡ çµæœã®è§£é‡ˆã¨è§£èª¬", open=False):
        with gr.Tabs():
            with gr.TabItem("ãƒ™ã‚¤ã‚ºåˆ†æã®è§£èª¬"):
                gr.Markdown(text_about_baysian_analysis)
            with gr.TabItem("ã‚«ã‚¤äºŒä¹—æ¤œå®šã®è§£èª¬"):
                gr.Markdown(text_about_chi_squared_test)
            with gr.TabItem("æ¤œå‡ºåŠ›åˆ†æã¨ã¯ï¼Ÿ"):
                gr.Markdown(text_about_power_analysis)
            with gr.TabItem("ãƒ™ã‚¤ã‚º vs ã‚«ã‚¤äºŒä¹—"):
                gr.Markdown(text_bayesien_vs_chi_squared)
    with gr.Accordion("ğŸ”¢ ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®è¨ˆç®—", open=False):
        with gr.Row():
            with gr.Column():
                power_analysis_inputs = {
                    "baseline_cvr": gr.Number(
                        0.053,
                        minimum=0.0,
                        maximum=1.0,
                        step=0.001,
                        label="ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³CVR",
                    ),
                    "lift": gr.Number(
                        0.01,
                        minimum=-1.0,
                        maximum=1.0,
                        step=0.001,
                        label="æ¤œå‡ºã—ãŸã„CVRã®å¢—åŠ é‡",
                    ),
                    "alpha": gr.Number(
                        0.05,
                        minimum=0.01,
                        maximum=0.5,
                        step=0.01,
                        label="æœ‰æ„æ°´æº–(Alpha)",
                    ),
                    "power": gr.Number(
                        0.80,
                        minimum=0.5,
                        maximum=0.99,
                        step=0.01,
                        label="æ¤œå‡ºåŠ›(1-Beta)",
                    ),
                    "ratio": gr.Number(
                        0.10,
                        minimum=0.0,
                        maximum=1000,
                        step=0.01,
                        label="Aç¾¤ã«å¯¾ã™ã‚‹Bç¾¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºæ¯”",
                    ),
                }
            with gr.Column():
                power_analysis_sample_size_a = gr.Number(label="A ç¾¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º")
                power_analysis_sample_size_b = gr.Number(label="B ç¾¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º")
                with gr.Accordion("åˆ†æçµæœã®è©³ç´°", open=False):
                    gr.Markdown("Cohen's h ã§åŠ¹æœé‡ã‚’è¨ˆç®—ã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™")
                    power_analysis_output = gr.DataFrame()

    # --- ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ ---
    chisq_event = start_button.click(
        fn=run_chisquared_test,
        inputs=[
            n_a,
            conversion_a,
            n_b,
            conversion_b,
        ],
        outputs=[
            chisq_result_df,
        ],
    )
    chisq_event.then(
        fn=run_bayesian_analysis,
        inputs=[
            n_a,
            conversion_a,
            n_b,
            conversion_b,
            n_sampling,
            n_tune,
            n_chains,
            random_seed,
            hdi_prob,
        ],
        outputs=[
            distribution,
            model_img,
            trace_plot,
            params,
        ],
    )
    gr.on(
        triggers=[x.change for x in power_analysis_inputs.values()],
        fn=run_power_analysis,
        inputs=[x for x in power_analysis_inputs.values()],
        outputs=[
            power_analysis_sample_size_a,
            power_analysis_sample_size_b,
            power_analysis_output,
        ],
    )

if __name__ == "__main__":
    demo.launch(
        share=False,
    )
